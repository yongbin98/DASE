{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "598ee996",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: 지정된 모듈을 찾을 수 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22816\\2845993539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdaBoostRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DASE\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DASE\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m from .utils._tags import (\n\u001b[0;32m     19\u001b[0m     \u001b[0m_DEFAULT_TAGS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DASE\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m from .validation import (\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DASE\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mthreadpoolctl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DASE\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    439\u001b[0m \"\"\"\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DASE\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m from scipy._lib._util import (check_random_state, MapWrapper,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DASE\\lib\\site-packages\\scipy\\spatial\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mkdtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mckdtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mqhull\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_spherical_voronoi\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSphericalVoronoi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_plotutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: 지정된 모듈을 찾을 수 없습니다."
     ]
    }
   ],
   "source": [
    "import os, psutil\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV,KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVR\n",
    "import sklearn.preprocessing\n",
    "from pycaret.regression import *\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import smogn\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def loadMatlabData():\n",
    "    fileName = os.getcwd() + '/DASE/daseData.mat'\n",
    "    \n",
    "    ###============= Load Matlab files\n",
    "    contentsMat = sio.loadmat(fileName)\n",
    "    x_train = contentsMat['x_train']\n",
    "    y_train = contentsMat['y_train']\n",
    "    x_test = contentsMat['x_test']\n",
    "    y_test = contentsMat['y_test']\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def loadxlsxData():\n",
    "    filename = os.getcwd() + '/DASE/KOBIO_data_final(changed).xlsx'\n",
    "    \n",
    "    ###============= Load xlsx files\n",
    "    df = pd.read_excel(filename)    \n",
    "    return df\n",
    "\n",
    "def loadbestPred(i):    \n",
    "    filename = os.getcwd() + '/DASE/result/best_pred'+str(i+1)+'.mat'\n",
    "    \n",
    "    contentsMat = sio.loadmat(filename)\n",
    "    x_train = contentsMat['x_train']\n",
    "    y_train = contentsMat['y_train']\n",
    "    x_test = contentsMat['x_test']\n",
    "    y_test = contentsMat['y_test']\n",
    "    y_pred = contentsMat['y_pred']\n",
    "    x_valid = contentsMat['x_valid']\n",
    "    y_valid = contentsMat['y_valid']\n",
    "    \n",
    "    return x_train, x_valid, x_test, y_train, y_valid, y_test, y_pred\n",
    "        \n",
    "def standarizeInput(x_train, x_valid, x_test, y_train, y_valid, y_test):    \n",
    "    key = ['AGE','BWT','HGT','BMI','CIGP','CIGY','PTGA','PHGA','ESR','CRP','BTIME','CDOSE','RF','ACCP']\n",
    "    \n",
    "    ssl = sklearn.preprocessing.StandardScaler()    \n",
    "    ssl.fit(x_train[key])\n",
    "    x_train[key] = ssl.transform(x_train[key])\n",
    "    x_valid[key] = ssl.transform(x_valid[key])    \n",
    "    x_test[key] = ssl.transform(x_test[key])    \n",
    "    \n",
    "    ssl = sklearn.preprocessing.MinMaxScaler()    \n",
    "    ssl.fit(x_train[key])\n",
    "    x_train[key] = ssl.transform(x_train[key])\n",
    "    x_valid[key] = ssl.transform(x_valid[key])\n",
    "    x_test[key] = ssl.transform(x_test[key])\n",
    "        \n",
    "    return x_train, x_valid, x_test, y_train, y_valid, y_test\n",
    "\n",
    "def standarizeInput4(x_train, x_test, y_train, y_test):     \n",
    "    key = ['AGE','BWT','HGT','BMI','CIGP','CIGY','PTGA','PHGA','ESR','CRP','BTIME','CDOSE','RF','ACCP']\n",
    "    \n",
    "    ssl = sklearn.preprocessing.StandardScaler()    \n",
    "    ssl.fit(x_train[key])\n",
    "    x_train[key] = ssl.transform(x_train[key])\n",
    "    x_test[key] = ssl.transform(x_test[key])    \n",
    "    \n",
    "    ssl = sklearn.preprocessing.MinMaxScaler()    \n",
    "    ssl.fit(x_train[key])\n",
    "    x_train[key] = ssl.transform(x_train[key])\n",
    "    x_test[key] = ssl.transform(x_test[key])\n",
    "        \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def applySMOGN(x_train, y_train):    \n",
    "    df_smogn = pd.concat([x_train, y_train], axis=1)\n",
    "    smogned = smogn.smoter(\n",
    "        data=df_smogn.reset_index(drop=True),\n",
    "        y='DASE',\n",
    "        k=5,\n",
    "        pert=0.05,\n",
    "        samp_method='extreme',\n",
    "        rel_thres=0.9,\n",
    "        rel_method='auto',\n",
    "        rel_xtrm_type='low',\n",
    "        rel_coef=0.9\n",
    "    )\n",
    "\n",
    "    x_train_smogned = smogned.drop(\"DASE\", axis=1).values\n",
    "    y_train_smogned = smogned[\"DASE\"].values\n",
    "\n",
    "    return x_train_smogned, y_train_smogned\n",
    "    # return x_train,y_train\n",
    "\n",
    "def log_cosh_loss(y_true, y_pred):\n",
    "    error = y_pred - y_true\n",
    "    loss = tf.math.log1p(tf.exp(2 * error)) - 2 * tf.math.log(2.0)\n",
    "    return tf.reduce_mean(loss, axis=-1)\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    quadratic_part = K.minimum(K.abs(error), delta)\n",
    "    linear_part = K.abs(error) - quadratic_part\n",
    "    loss = 0.5 * K.square(quadratic_part) + delta * linear_part\n",
    "    return K.mean(loss, axis=-1)\n",
    "\n",
    "def modelDNN(x_train, x_valid, x_test, y_train, y_valid, y_test):\n",
    "    \n",
    "    input = tf.keras.layers.Input(shape=(111,1,1))\n",
    "    x = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,1), strides = (1), activation = 'relu', padding='valid', name='CV1')(input)\n",
    "    x = tf.keras.layers.AveragePooling2D(pool_size = (3,1), strides = (2,1), name='AP1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3,1), strides = (1), activation = 'relu', padding='valid', name='CV2')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(pool_size = (3,1), strides = (2,1), name='AP2')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3,1), strides = (1), activation = 'relu', padding='valid', name='CV3')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D(pool_size = (3,1), strides = (2,1), name='AP3')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Flatten(name='Flatten')(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(128, activation='relu', name='FC0')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='FC1')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu', name='FC2')(x)\n",
    "    output = tf.keras.layers.Dense(1, name='Output')(x)\n",
    "    \n",
    "    model= tf.keras.models.Model(inputs=input, outputs=output)\n",
    "    model.summary()\n",
    "    \n",
    "    callback_list = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='mse', mode='min', verbose=0, patience=10),\n",
    "            tf.keras.callbacks.ModelCheckpoint(filepath=os.getcwd()+'/DASE/model/model_dnn.h5', monitor='mse', mode='min', verbose=0, save_best_only=True, save_weights_only=True),\n",
    "        ]\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,loss='mse',metrics=['mae','mse'])\n",
    "    x_train = tf.expand_dims(x_train, axis=-1) \n",
    "    x_valid = tf.expand_dims(x_valid, axis=-1) \n",
    "    model.fit(x_train, y_train, batch_size=16, epochs=300, validation_data = (x_valid,y_valid), callbacks=callback_list)\n",
    "    x_test = tf.expand_dims(x_test, axis=-1) \n",
    "    y_pred = model.predict(x_test)\n",
    "    evaluation(y_test,y_pred)\n",
    "    return y_pred\n",
    "    \n",
    "def modelGridSearch(x_train, x_test, y_train, y_test):\n",
    "    param_grid = {\n",
    "    'max_depth': [5, 7, 9, 11],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.9, 1],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error' , verbose = 2)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    \n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(x_test)\n",
    "    print(best_params)\n",
    "    evaluation(y_test,y_pred)\n",
    "    return y_pred\n",
    "\n",
    "def modelPycaret(x_train, x_test, y_train, y_test):\n",
    "    df_train = pd.concat([x_train, y_train], axis=1)\n",
    "    df_test = pd.concat([x_test, y_test], axis=1)\n",
    "    setup(data = df_train.reset_index(drop=True),test_data=df_test.reset_index(drop=True),target='DASE',session_id=99,index=False)\n",
    "    \n",
    "    best_model = compare_models(n_select=25,sort='MAE',include=['lr','lasso','ridge','en','lar','llar','omp','br','ard','par','ransac','tr','huber','kr','svm','knn','dt','rf','et','ada','gbr','mlp','xgboost','lightgbm','catboost'])\n",
    "    print(best_model)\n",
    "    return df_test\n",
    "    \n",
    "def modelXGB(x_train, x_test, y_train, y_test):    \n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror',n_estimators=100, learning_rate=0.05, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7).fit(x_train,y_train)\n",
    "    \n",
    "    train_pred = model.predict(x_train.squeeze())\n",
    "    test_pred = model.predict(x_test.squeeze())\n",
    "    print(model)\n",
    "    evaluation(y_test.squeeze(),test_pred)\n",
    "    return test_pred, train_pred\n",
    "\n",
    "def modelRF(x_train, x_test, y_train, y_test):    \n",
    "    model = RandomForestRegressor().fit(x_train,y_train)\n",
    "    \n",
    "    train_pred = model.predict(x_train.squeeze())\n",
    "    test_pred = model.predict(x_test.squeeze())\n",
    "    print(model)\n",
    "    evaluation(y_test.squeeze(),test_pred)\n",
    "    return test_pred, train_pred\n",
    "\n",
    "def modelGBR(x_train, x_test, y_train, y_test):        \n",
    "    model = GradientBoostingRegressor().fit(x_train.squeeze(),y_train.squeeze())\n",
    "    \n",
    "    train_pred = model.predict(x_train.squeeze())\n",
    "    test_pred = model.predict(x_test.squeeze())\n",
    "    print(model)\n",
    "    evaluation(y_test.squeeze(),test_pred)\n",
    "    return test_pred, train_pred\n",
    "\n",
    "def modelADA(x_train, x_test, y_train, y_test):    \n",
    "    model = AdaBoostRegressor().fit(x_train.squeeze(),y_train.squeeze())\n",
    "    \n",
    "    train_pred = model.predict(x_train.squeeze())\n",
    "    test_pred = model.predict(x_test.squeeze())\n",
    "    print(model)\n",
    "    evaluation(y_test.squeeze(),test_pred)\n",
    "    return test_pred, train_pred\n",
    "\n",
    "def modelLGB(x_train, x_test, y_train, y_test):    \n",
    "    model = lgb.LGBMRegressor().fit(x_train.squeeze(),y_train.squeeze())\n",
    "    \n",
    "    train_pred = model.predict(x_train.squeeze())\n",
    "    test_pred = model.predict(x_test.squeeze())\n",
    "    print(model)\n",
    "    evaluation(y_test.squeeze(),test_pred)\n",
    "    return test_pred, train_pred\n",
    "\n",
    "def modelSVR(x_train, x_test, y_train, y_test):    \n",
    "    \n",
    "    model = SVR().fit(x_train.squeeze(),y_train.squeeze())\n",
    "    \n",
    "    train_pred = model.predict(x_train.squeeze())\n",
    "    test_pred = model.predict(x_test.squeeze())\n",
    "    print(model)\n",
    "    evaluation(y_test.squeeze(),test_pred)\n",
    "    return test_pred, train_pred\n",
    "\n",
    "def modelMLMerge(x_train, x_valid, x_test, y_train, y_valid, y_test):\n",
    "    input1 = tf.keras.layers.Input(shape=(1))  \n",
    "    input2 = tf.keras.layers.Input(shape=(1))  \n",
    "    input3 = tf.keras.layers.Input(shape=(1))  \n",
    "    input4 = tf.keras.layers.Input(shape=(1))  \n",
    "        \n",
    "    x = tf.keras.layers.Concatenate()([input1,input2,input3,input4])\n",
    "    x = tf.keras.layers.Dense(32, activation='relu', name='FC1')(x)\n",
    "    # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(16, activation='relu', name='FC2')(x)\n",
    "    x = tf.keras.layers.Dense(8, activation='relu', name='FC3')(x)\n",
    "    # x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(4, activation='relu', name='FC4')(x)\n",
    "    output = tf.keras.layers.Dense(1, name='Output')(x)\n",
    "    \n",
    "    callback_list = [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10),\n",
    "            tf.keras.callbacks.ModelCheckpoint(filepath=os.getcwd()+'/DASE/model/model_dnn.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True, save_weights_only=True),\n",
    "        ]\n",
    "    \n",
    "    model= tf.keras.models.Model(inputs=[input1,input2,input3,input4], outputs=output)\n",
    "    model.summary()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,loss='mae',metrics=['mae'])\n",
    "    model.fit(x_train, y_train, batch_size=16, epochs=300, validation_data = (x_valid,y_valid), callbacks=callback_list)\n",
    "    model.load_weights(filepath=os.getcwd()+'/DASE/model/model_dnn.h5')        \n",
    "    y_pred = model.predict(x_test)\n",
    "    evaluation(y_test,y_pred)\n",
    "    return y_pred\n",
    "\n",
    "def modelTrain(x_train, x_valid, x_test, y_train, y_valid, y_test,i):    \n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror',n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7\n",
    "                             )\n",
    "    model.fit(x_train,y_train, eval_set=[(x_train,y_train),(x_valid.values,y_valid.values)],early_stopping_rounds=20)\n",
    "    y_pred = model.predict(x_test.values)\n",
    "    evaluation(y_test.values,y_pred)\n",
    "    model.save_model(os.getcwd() + '/DASE/model/best_model'+str(i)+'.h5')\n",
    "    # pltFeature(model.feature_importances_, x_test.columns)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "# def randomNoise4(x_train, x_test, y_train, y_test):\n",
    "    \n",
    "    \n",
    "#     return x_train, x_test, y_train, y_test\n",
    "\n",
    "def evaluation(y_test,y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2Score = r2_score(y_test,y_pred)\n",
    "    print(\"MAE :\", mae)\n",
    "    print(\"MSE :\", mse)\n",
    "    print(\"RMSE :\", rmse)    \n",
    "    print(\"R2 :\", r2Score)\n",
    "    \n",
    "def qunatileSplit(x_data,y_data,df_columns):  \n",
    "    sorted_indices = np.argsort(y_data)\n",
    "    sorted_x_data = x_data[sorted_indices]\n",
    "    sorted_y_data = y_data[sorted_indices]\n",
    "    num_quantiles = 10\n",
    "    \n",
    "    bins = np.linspace(0, len(y_data), num_quantiles+1, dtype=int)\n",
    "    bin_indices = np.digitize(range(len(y_data)), bins)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(sorted_x_data, sorted_y_data, test_size=0.2, stratify=bin_indices, random_state=99)\n",
    "    \n",
    "    x_train = numpyToDataFrame(x_train,df_columns[1:-1])\n",
    "    x_test = numpyToDataFrame(x_test,df_columns[1:-1])\n",
    "    y_train = numpyToDataFrame(y_train,['DASE'])\n",
    "    y_test = numpyToDataFrame(y_test,['DASE'])\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def numpyToDataFrame(x,column):    \n",
    "    df_x = pd.DataFrame(x)\n",
    "    df_x.columns = column\n",
    "    return df_x\n",
    "\n",
    "def pltFeature(importance, feature_names):\n",
    "    indices = importance.argsort()[::-1]\n",
    "    sorted_importance = importance[indices]\n",
    "    sorted_feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(sorted_importance)), sorted_importance)\n",
    "    plt.xticks(range(len(sorted_importance)), sorted_feature_names, rotation=90)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df = loadxlsxData()\n",
    "x_data = df.drop(['DASE','Subject'],axis=1)\n",
    "y_data = df['DASE']\n",
    "x_train, x_test, y_train, y_test = qunatileSplit(x_data.values,y_data.values,df.columns)\n",
    "\n",
    "### grid search\n",
    "# y_pred = modelGridSearch(x_train, x_test, y_train, y_test)\n",
    "\n",
    "### single model dnn\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_data.values, y_data.values, test_size=0.2, shuffle=True, random_state=99)\n",
    "# x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, shuffle=True, random_state=99)\n",
    "\n",
    "# x_train = numpyToDataFrame(x_train,df.columns[1:-1])\n",
    "# x_valid = numpyToDataFrame(x_valid,df.columns[1:-1])\n",
    "# x_test = numpyToDataFrame(x_test,df.columns[1:-1])\n",
    "# y_train = numpyToDataFrame(y_train,['DASE'])\n",
    "# y_valid = numpyToDataFrame(y_valid,['DASE'])\n",
    "# y_test = numpyToDataFrame(y_test,['DASE'])\n",
    "\n",
    "# x_train, x_valid, x_test, y_train, y_valid, y_test = standarizeInput(x_train, x_valid, x_test, y_train, y_valid, y_test)\n",
    "# x_train, y_train = applySMOGN(x_train, y_train)\n",
    "\n",
    "### single model ML\n",
    "# x_train, x_valid, y_train, y_valid = qunatileSplit(x_train.values.squeeze(),y_train.values.squeeze(),df.columns)\n",
    "\n",
    "# x_train, x_valid, x_test, y_train, y_valid, y_test = standarizeInput(x_train, x_valid, x_test, y_train, y_valid, y_test)\n",
    "# x_train, y_train = applySMOGN(x_train, y_train)\n",
    "\n",
    "x_train, x_test, y_train, y_test = standarizeInput4(x_train, x_test, y_train, y_test)\n",
    "# x_train, y_train = applySMOGN(x_train, y_train)\n",
    "y_pred = modelPycaret(x_train, x_test, y_train, y_test)\n",
    "# y_pred = modelGridSearch(x_train, x_test, y_train, y_test)\n",
    "\n",
    "# valid_pred3, train_pred3 = modelXGB(x_train, x_valid, y_train, y_valid)\n",
    "# valid_pred4, train_pred4 = modelGBR(x_train, x_valid, y_train, y_valid)\n",
    "# valid_pred5, train_pred5 = modelRF(x_train, x_valid, y_train, y_valid)\n",
    "# valid_pred6, train_pred6 = modelLGB(x_train, x_valid, y_train, y_valid)\n",
    "\n",
    "# xx_valid = [valid_pred3,valid_pred4,valid_pred5,valid_pred6]\n",
    "# yy_valid = y_valid\n",
    "\n",
    "# test_pred3, train_pred3 = modelXGB(x_train, x_test, y_train, y_test)\n",
    "# test_pred4, train_pred4 = modelGBR(x_train, x_test, y_train, y_test)\n",
    "# test_pred5, train_pred5 = modelRF(x_train, x_test, y_train, y_test)\n",
    "# test_pred6, train_pred6 = modelLGB(x_train, x_test, y_train, y_test)\n",
    "\n",
    "# xx_test = [test_pred3,test_pred4,test_pred5,test_pred6]\n",
    "# yy_test = y_test\n",
    "\n",
    "# xx_train = [train_pred3,train_pred4,train_pred5,train_pred6]\n",
    "# yy_train = y_train\n",
    "\n",
    "# y_preds = modelMLMerge(xx_train, xx_valid, xx_test, yy_train, yy_valid, yy_test)\n",
    "\n",
    "# y_preds = test_pred3\n",
    "# yy_test = y_test\n",
    "# sio.savemat('./DASE/result/merge.mat',{'y_preds':y_preds,'yy_test':yy_test.values})\n",
    "\n",
    "### 5 fold\n",
    "# x_train = x_train.values\n",
    "# y_train = y_train.values\n",
    "# kf = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "# i = 1\n",
    "\n",
    "# for train_index, valid_index in kf.split(x_train):\n",
    "#     kfX_train, kfX_valid = x_train[train_index], x_train[valid_index]\n",
    "#     kfY_train, kfY_valid = y_train[train_index], y_train[valid_index]\n",
    "#     dfX_test, dfY_test = x_test.copy(), y_test.copy()\n",
    "    \n",
    "#     dfX_train = numpyToDataFrame(kfX_train,df.columns[1:-1])\n",
    "#     dfX_valid = numpyToDataFrame(kfX_valid,df.columns[1:-1])\n",
    "#     dfY_train = numpyToDataFrame(kfY_train,['DASE'])\n",
    "#     dfY_valid = numpyToDataFrame(kfY_valid,['DASE'])\n",
    "            \n",
    "#     normX_train, normX_valid, normX_test, normY_train, normY_valid, normY_test = standarizeInput(dfX_train, dfX_valid, dfX_test, dfY_train, dfY_valid, dfY_test)\n",
    "    \n",
    "#     augX_train, augY_train = applySMOGN(normX_train, normY_train)\n",
    "#     valid_pred3, train_pred3 = modelXGB(augX_train, normX_valid, augY_train, normY_valid)\n",
    "#     valid_pred4, train_pred4 = modelGBR(augX_train, normX_valid, augY_train, normY_valid)\n",
    "#     valid_pred5, train_pred5 = modelRF(augX_train, normX_valid, augY_train, normY_valid)\n",
    "#     valid_pred6, train_pred6 = modelLGB(augX_train, normX_valid, augY_train, normY_valid)\n",
    "\n",
    "#     mergeX_valid = [valid_pred3,valid_pred4,valid_pred5,valid_pred6]\n",
    "#     mergeY_valid = normY_valid\n",
    "        \n",
    "#     test_pred3, train_pred3 = modelXGB(augX_train, normX_test, augY_train, normY_test)\n",
    "#     test_pred4, train_pred4 = modelGBR(augX_train, normX_test, augY_train, normY_test)\n",
    "#     test_pred5, train_pred5 = modelRF(augX_train, normX_test, augY_train, normY_test)\n",
    "#     test_pred6, train_pred6 = modelLGB(augX_train, normX_test, augY_train, normY_test)\n",
    "\n",
    "#     mergeX_test = [test_pred3,test_pred4,test_pred5,test_pred6]\n",
    "#     mergeY_test = normY_test\n",
    "\n",
    "#     mergeX_train = [train_pred3,train_pred4,train_pred5,train_pred6]\n",
    "#     mergeY_train = augY_train\n",
    "\n",
    "#     y_preds = modelMLMerge(mergeX_train, mergeX_valid, mergeX_test, mergeY_train, mergeY_valid, mergeY_test)\n",
    "    \n",
    "#     sio.savemat(os.getcwd()+'/DASE/result/best_pred'+str(i)+'.mat',{'x_train' : mergeX_train, 'y_train' : mergeY_train, 'x_test' : mergeX_test, 'y_test' : mergeY_test.values, 'y_pred' : y_preds,\n",
    "#                                                               'x_valid' : mergeX_valid, 'y_valid' : mergeY_valid.values})\n",
    "#     i=i+1\n",
    "\n",
    "### ensemble softvoting\n",
    "\n",
    "# vPreds = []\n",
    "# for i in range(5):\n",
    "#     x_train, x_valid, x_test, y_train, y_valid, y_test, y_pred = loadbestPred(i)    \n",
    "#     vPreds.append(y_pred)\n",
    "#     evaluation(y_test,y_pred)\n",
    "    \n",
    "# vPred = (vPreds[0] + vPreds[1] + vPreds[2] + vPreds[3] + vPreds[4])/5\n",
    "# evaluation(y_test,vPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa2fa35",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (728349862.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\AW_Lab_LYB\\AppData\\Local\\Temp\\ipykernel_22816\\728349862.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    jupyter kernelspec list\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DASE",
   "language": "python.exe",
   "name": "dase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
